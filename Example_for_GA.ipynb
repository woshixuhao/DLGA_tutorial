{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "dcd27d7c-f9b2-4fd3-ab09-0e1f9d833798",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import scipy.io as scio\n",
    "import matplotlib.pyplot as plt\n",
    "import torch.nn.functional as F\n",
    "import torch\n",
    "import random\n",
    "import os\n",
    "from torch.autograd import Variable\n",
    "from torch.nn import Linear,Tanh,Sequential\n",
    "import torch.nn as nn\n",
    "import math\n",
    "import scipy.io as scio\n",
    "import time\n",
    "from tqdm import tqdm\n",
    "import pickle\n",
    "import heapq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "640d8825-46fd-444f-9fe2-51b9e6502287",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Rational(torch.nn.Module):\n",
    "    def __init__(self,\n",
    "                 Data_Type = torch.float32,\n",
    "                 Device    = torch.device('cpu')):\n",
    "        # This activation function is based on the following paper:\n",
    "        # Boulle, Nicolas, Yuji Nakatsukasa, and Alex Townsend. \"Rational neural\n",
    "        # networks.\" arXiv preprint arXiv:2004.01902 (2020).\n",
    "\n",
    "        super(Rational, self).__init__()\n",
    "\n",
    "        # Initialize numerator and denominator coefficients to the best\n",
    "        # rational function approximation to ReLU. These coefficients are listed\n",
    "        # in appendix A of the paper.\n",
    "        self.a = torch.nn.parameter.Parameter(\n",
    "                        torch.tensor((1.1915, 1.5957, 0.5, .0218),\n",
    "                                     dtype = Data_Type,\n",
    "                                     device = Device))\n",
    "        self.a.requires_grad_(True)\n",
    "\n",
    "        self.b = torch.nn.parameter.Parameter(\n",
    "                        torch.tensor((2.3830, 0.0, 1.0),\n",
    "                                     dtype = Data_Type,\n",
    "                                     device = Device))\n",
    "        self.b.requires_grad_(True)\n",
    "\n",
    "    def forward(self, X : torch.tensor):\n",
    "        \"\"\" This function applies a rational function to each element of X.\n",
    "        ------------------------------------------------------------------------\n",
    "        Arguments:\n",
    "        X: A tensor. We apply the rational function to every element of X.\n",
    "        ------------------------------------------------------------------------\n",
    "        Returns:\n",
    "        Let N(x) = sum_{i = 0}^{3} a_i x^i and D(x) = sum_{i = 0}^{2} b_i x^i.\n",
    "        Let R = N/D (ignoring points where D(x) = 0). This function applies R\n",
    "        to each element of X and returns the resulting tensor. \"\"\"\n",
    "\n",
    "        # Create aliases for self.a and self.b. This makes the code cleaner.\n",
    "        a = self.a\n",
    "        b = self.b\n",
    "\n",
    "        # Evaluate the numerator and denominator. Because of how the * and +\n",
    "        # operators work, this gets applied element-wise.\n",
    "        N_X = a[0] + X*(a[1] + X*(a[2] + a[3]*X))\n",
    "        D_X = b[0] + X*(b[1] + b[2]*X)\n",
    "\n",
    "        # Return R = N_X/D_X. This is also applied element-wise.\n",
    "        return N_X/D_X\n",
    "\n",
    "class Sin(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Sin, self).__init__()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = torch.sin(x)\n",
    "        return x\n",
    "        \n",
    "class NN(torch.nn.Module):\n",
    "    def __init__(self,\n",
    "                 Num_Hidden_Layers   : int          = 3,\n",
    "                 Neurons_Per_Layer   : int          = 20,   # Neurons in each Hidden Layer\n",
    "                 Input_Dim           : int          = 1,    # Dimension of the input\n",
    "                 Output_Dim          : int          = 1,    # Dimension of the output\n",
    "                 Data_Type           : torch.dtype  = torch.float32,\n",
    "                 Device              : torch.device = torch.device('cpu'),\n",
    "                 Activation_Function : str          = \"Tanh\",\n",
    "                 Batch_Norm          : bool         = False):\n",
    "        # For the code below to work, Num_Hidden_Layers, Neurons_Per_Layer,\n",
    "        # Input_Dim, and Output_Dim must be positive integers.\n",
    "        assert(Num_Hidden_Layers   > 0), \"Num_Hidden_Layers must be positive. Got %du\" % Num_Hidden_Layers;\n",
    "        assert(Neurons_Per_Layer   > 0), \"Neurons_Per_Layer must be positive. Got %u\" % Neurons_Per_Layer;\n",
    "        assert(Input_Dim           > 0), \"Input_Dim must be positive. Got %u\"  % Input_Dim;\n",
    "        assert(Output_Dim          > 0), \"Output_Dim must be positive. Got %u\" % Output_Dim;\n",
    "\n",
    "        super(NN, self).__init__()\n",
    "\n",
    "        # Define object attributes.\n",
    "        self.Input_Dim          : int  = Input_Dim\n",
    "        self.Output_Dim         : int  = Output_Dim\n",
    "        self.Num_Hidden_Layers  : int  = Num_Hidden_Layers\n",
    "        self.Batch_Norm         : bool = Batch_Norm\n",
    "\n",
    "        # Initialize the Layers. We hold all layers in a ModuleList.\n",
    "        self.Layers = torch.nn.ModuleList()\n",
    "\n",
    "        # Initialize Batch Normalization, if we're doing that.\n",
    "        if(Batch_Norm == True):\n",
    "            self.Norm_Layer = torch.nn.BatchNorm1d(\n",
    "                                    num_features = Input_Dim,\n",
    "                                    dtype        = Data_Type,\n",
    "                                    device       = Device)\n",
    "\n",
    "        # Append the first hidden layer. The domain of this layer is\n",
    "        # R^{Input_Dim}. Thus, in_features = Input_Dim. Since this is a hidden\n",
    "        # layer, its co-domain is R^{Neurons_Per_Layer}. Thus, out_features =\n",
    "        # Neurons_Per_Layer.\n",
    "        self.Layers.append(torch.nn.Linear(\n",
    "                                in_features  = Input_Dim,\n",
    "                                out_features = Neurons_Per_Layer,\n",
    "                                bias         = True ).to(dtype = Data_Type, device = Device))\n",
    "\n",
    "        # Now append the rest of the hidden layers. Each maps from\n",
    "        # R^{Neurons_Per_Layer} to itself. Thus, in_features = out_features =\n",
    "        # Neurons_Per_Layer. We start at i = 1 because we already created the\n",
    "        # 1st hidden layer.\n",
    "        for i in range(1, Num_Hidden_Layers):\n",
    "            self.Layers.append(torch.nn.Linear(\n",
    "                                    in_features  = Neurons_Per_Layer,\n",
    "                                    out_features = Neurons_Per_Layer,\n",
    "                                    bias         = True ).to(dtype = Data_Type, device = Device))\n",
    "\n",
    "        # Now, append the Output Layer, which has Neurons_Per_Layer input\n",
    "        # features, but only Output_Dim output features.\n",
    "        self.Layers.append(torch.nn.Linear(\n",
    "                                in_features  = Neurons_Per_Layer,\n",
    "                                out_features = Output_Dim,\n",
    "                                bias         = True ).to(dtype = Data_Type, device = Device))\n",
    "\n",
    "        # Initialize the weight matrices, bias vectors in the network.\n",
    "        if(Activation_Function == \"Tanh\" or Activation_Function == \"Rational\"):\n",
    "            Gain : float = 0\n",
    "            if  (Activation_Function == \"Tanh\"):\n",
    "                Gain = 5./3.\n",
    "            elif(Activation_Function == \"Rational\"):\n",
    "                Gain = 1.41\n",
    "\n",
    "            for i in range(self.Num_Hidden_Layers + 1):\n",
    "                torch.nn.init.xavier_normal_(self.Layers[i].weight, gain = Gain)\n",
    "                torch.nn.init.zeros_(self.Layers[i].bias)\n",
    "\n",
    "        elif(Activation_Function == \"Sin\"):\n",
    "            # The SIREN paper suggests initializing the elements of every weight\n",
    "            # matrix (except for the first one) by sampling a uniform\n",
    "            # distribution over [-c/root(n), c/root(n)], where c > root(6),\n",
    "            # and n is the number of neurons in the layer. I use c = 3 > root(6).\n",
    "            #\n",
    "            # Further, for simplicity, I initialize each bias vector to be zero.\n",
    "            a : float = 3./math.sqrt(Neurons_Per_Layer)\n",
    "            for i in range(0, self.Num_Hidden_Layers + 1):\n",
    "                torch.nn.init.uniform_( self.Layers[i].weight, -a, a)\n",
    "                torch.nn.init.zeros_(   self.Layers[i].bias)\n",
    "\n",
    "        # Finally, set the Network's activation functions.\n",
    "        self.Activation_Functions = torch.nn.ModuleList()\n",
    "        if  (Activation_Function == \"Tanh\"):\n",
    "            for i in range(Num_Hidden_Layers):\n",
    "                self.Activation_Functions.append(torch.nn.Tanh())\n",
    "        elif(Activation_Function == \"Sin\"):\n",
    "            for i in range(Num_Hidden_Layers):\n",
    "                self.Activation_Functions.append(Sin())\n",
    "        elif(Activation_Function == \"Rational\"):\n",
    "            for i in range(Num_Hidden_Layers):\n",
    "                self.Activation_Functions.append(Rational(Data_Type = Data_Type, Device = Device))\n",
    "        else:\n",
    "            print(\"Unknown Activation Function. Got %s\" % Activation_Function)\n",
    "            print(\"Thrown by Neural_Network.__init__. Aborting.\")\n",
    "            exit();\n",
    "\n",
    "\n",
    "\n",
    "    def forward(self, X : torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\" Forward method for the NN class. Note that the user should NOT call\n",
    "        this function directly. Rather, they should call it through the __call__\n",
    "        method (using the NN object like a function), which is part of the\n",
    "        module class and calls forward.\n",
    "\n",
    "        ------------------------------------------------------------------------\n",
    "        Arguments:\n",
    "\n",
    "        X: A batch of inputs. This should be a B by Input_Dim tensor, where B\n",
    "        is the batch size. The ith row of X should hold the ith input.\n",
    "\n",
    "        ------------------------------------------------------------------------\n",
    "        Returns:\n",
    "\n",
    "        If X is a B by Input_Dim tensor, then the output of this function is a\n",
    "        B by Output_Dim tensor, whose ith row holds the value of the network\n",
    "        applied to the ith row of X. \"\"\"\n",
    "\n",
    "        # If we are using batch normalization, then normalize the inputs.\n",
    "        if(self.Batch_Norm == True):\n",
    "            X = self.Norm_Layer(X);\n",
    "\n",
    "        # Pass X through the hidden layers. Each has an activation function.\n",
    "        for i in range(0, self.Num_Hidden_Layers):\n",
    "            X = self.Activation_Functions[i](self.Layers[i](X));\n",
    "\n",
    "        # Pass through the last layer (with no activation function) and return.\n",
    "        return self.Layers[self.Num_Hidden_Layers](X);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5d31478b-8074-4b96-bb86-662301cc3c19",
   "metadata": {},
   "outputs": [],
   "source": [
    "def random_data(total, choose,choose_validate,x,t,un,x_num,t_num,random_seed=525):\n",
    "    random.seed(random_seed)\n",
    "    data=np.zeros(2)\n",
    "    h_data=np.zeros([total,1])\n",
    "    database=np.zeros([total,2])\n",
    "    num=0\n",
    "\n",
    "\n",
    "    for j in range(x_num):\n",
    "        for i in range(t_num):\n",
    "            data[0]=x[j]\n",
    "            data[1]=t[i]\n",
    "            h_data[num]=un[j,i]\n",
    "            database[num]=data\n",
    "            num+=1\n",
    "    state = np.random.get_state()\n",
    "    np.random.shuffle(database)\n",
    "    np.random.set_state(state)\n",
    "    np.random.shuffle(h_data)\n",
    "    h_data_choose = h_data[0:choose]\n",
    "    database_choose = database[0:choose]\n",
    "    h_data_validate = h_data[choose:choose + choose_validate]\n",
    "    database_validate = database[choose:choose + choose_validate]\n",
    "\n",
    "    h_data_choose = torch.from_numpy(h_data_choose.astype(np.float32))\n",
    "    database_choose = torch.from_numpy(database_choose.astype(np.float32))\n",
    "    h_data_validate = torch.from_numpy(h_data_validate.astype(np.float32))\n",
    "    database_validate = torch.from_numpy(database_validate.astype(np.float32))\n",
    "   \n",
    "    return h_data_choose,h_data_validate,database_choose,database_validate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "83ee3fb3-e28b-402e-9381-5d2dc94d73c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Generate_meta_data(Net,Equation_name, choose, noise_level, Load_state, x_low, x_up, t_low, t_up, nx=100,\n",
    "                       nt=100, ):\n",
    "    Net.load_state_dict(torch.load(f'model_save/{Equation_name}/{choose}_{noise_level}/{Load_state}.pkl'))\n",
    "    Net.eval()\n",
    "\n",
    "    x = torch.linspace(x_low, x_up, nx)\n",
    "    t = torch.linspace(t_low, t_up, nt)\n",
    "    total = nx * nt\n",
    "\n",
    "    num = 0\n",
    "    data = torch.zeros(2)\n",
    "    h_data = torch.zeros([total, 1])\n",
    "    database = torch.zeros([total, 2])\n",
    "    for j in range(nx):\n",
    "        for i in range(nt):\n",
    "            data[0] = x[j]\n",
    "            data[1] = t[i]\n",
    "            database[num] = data\n",
    "            num += 1\n",
    "\n",
    "    database = Variable(database, requires_grad=True).to(device)\n",
    "    PINNstatic=Net(database)\n",
    "\n",
    "    H_grad = torch.autograd.grad(outputs=PINNstatic.sum(), inputs=database, create_graph=True)[0]\n",
    "    Hx=H_grad[:,0].reshape(total,1)\n",
    "    Ht=H_grad[:,1].reshape(total,1)\n",
    "    Hxx=torch.autograd.grad(outputs=Hx.sum(), inputs=database,create_graph=True)[0][:,0].reshape(total,1)\n",
    "    Hxxx=torch.autograd.grad(outputs=Hxx.sum(), inputs=database,create_graph=True)[0][:,0].reshape(total,1)\n",
    "    Htt=torch.autograd.grad(outputs=Ht.sum(), inputs=database,create_graph=True)[0][:,1].reshape(total,1)\n",
    "    Theta=torch.concatenate([PINNstatic,Hx,Hxx,Hxxx,Ht,Htt],axis=1) \n",
    " \n",
    "    return Theta.cpu().data.numpy(),x.cpu().data.numpy(),t.cpu().data.numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "763ef319-ba23-443e-88df-8fa004e21839",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GA():\n",
    "    def __init__(self,x,t,epi,R):\n",
    "        self.max_length = 5\n",
    "        self.partial_prob = 0.6\n",
    "        self.genes_prob = 0.6\n",
    "        self.mutate_rate = 0.4\n",
    "        self.delete_rate = 0.5\n",
    "        self.add_rate = 0.4\n",
    "        self.pop_size = 400\n",
    "        self.n_generations = 100\n",
    "        self.u=R[:,0].reshape(R.shape[0],1)\n",
    "        self.u_x=R[:,1].reshape(R.shape[0],1)\n",
    "        self.u_xx=R[:,2].reshape(R.shape[0],1)\n",
    "        self.u_xxx=R[:,3].reshape(R.shape[0],1)\n",
    "        self.u_t=R[:,4].reshape(R.shape[0],1)\n",
    "        self.u_tt=R[:,5].reshape(R.shape[0],1)\n",
    "        self.x=x\n",
    "        self.t=t\n",
    "        self.dx=x[1]-x[0]\n",
    "        self.dt=t[1]-t[0]\n",
    "        self.nx=x.shape[0]\n",
    "        self.nt=t.shape[0]\n",
    "        self.total=self.nx*self.nt\n",
    "        self.epi=epi\n",
    "\n",
    "\n",
    "\n",
    "    def random_module(self):\n",
    "        genes_module=[]\n",
    "        for i in range(self.max_length):\n",
    "            a = random.randint(0, 3)\n",
    "            genes_module.append(a)\n",
    "            prob=random.uniform(0,1)\n",
    "            if prob>self.partial_prob:\n",
    "                break\n",
    "        return genes_module\n",
    "\n",
    "    def random_genome(self):\n",
    "        genes=[]\n",
    "\n",
    "        for i in range(self.max_length):\n",
    "            gene_random=GA.random_module(self)\n",
    "            genes.append(sorted(gene_random))\n",
    "            prob=random.uniform(0,1)\n",
    "            if prob>self.genes_prob:\n",
    "                break\n",
    "        return genes\n",
    "\n",
    "    def translate_DNA(self,gene):\n",
    "        gene_translate=np.ones([self.total,1])\n",
    "        length_penalty_coef=0\n",
    "        for k in range(len(gene)):\n",
    "            gene_module=gene[k]\n",
    "            length_penalty_coef+=len(gene_module)\n",
    "            module_out=np.ones([self.u.shape[0],self.u.shape[1]])\n",
    "            for i in gene_module:\n",
    "                if i==0:\n",
    "                    temp=self.u\n",
    "                if i==1:\n",
    "                    temp=self.u_x\n",
    "                if i==2:\n",
    "                    temp=self.u_xx\n",
    "                if i==3:\n",
    "                    temp=self.u_xxx\n",
    "                module_out*=temp\n",
    "            gene_translate=np.hstack((gene_translate,module_out))\n",
    "        gene_translate=np.delete(gene_translate,[0],axis=1)\n",
    "        return gene_translate,length_penalty_coef\n",
    "\n",
    "    def get_fitness(self,gene_translate,length_penalty_coef):\n",
    "        u_t=self.u_t\n",
    "        u, d, v = np.linalg.svd(np.hstack((u_t, gene_translate)), full_matrices=False)\n",
    "        coef_NN = v.T[:, -1] / (v.T[:, -1][0] + 1e-8)\n",
    "        coef=-coef_NN[1:].reshape(coef_NN.shape[0]-1,1)\n",
    "        res = u_t-np.dot(gene_translate,coef)\n",
    "\n",
    "        u_tt=self.u_tt\n",
    "        u, d, v = np.linalg.svd(np.hstack((u_tt, gene_translate)), full_matrices=False)\n",
    "        coef_NN = v.T[:, -1] / (v.T[:, -1][0] + 1e-8)\n",
    "        coef_tt=-coef_NN[1:].reshape(coef_NN.shape[0]-1,1)\n",
    "        res_tt = u_tt-np.dot(gene_translate,coef_tt)\n",
    "\n",
    "        MSE_true = np.sum(np.array(res) ** 2) / self.total\n",
    "        MSE_true_tt = np.sum(np.array(res_tt) ** 2) / self.total\n",
    "        \n",
    "        if MSE_true<MSE_true_tt:\n",
    "            name='u_t'\n",
    "            MSE=MSE_true+self.epi*length_penalty_coef\n",
    "            coef=coef\n",
    "            return coef,MSE,MSE_true,name\n",
    "\n",
    "\n",
    "        if MSE_true>MSE_true_tt:\n",
    "            name='u_tt'\n",
    "            MSE = MSE_true_tt + self.epi * length_penalty_coef\n",
    "            return coef_tt, MSE, MSE_true_tt, name\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    def cross_over(self):\n",
    "        Chrom, size_pop = self.Chrom,self.n_generations\n",
    "        Chrom1, Chrom2 = Chrom[::2], Chrom[1::2]\n",
    "        for i in range(int(size_pop / 2)):\n",
    "            n1= np.random.randint(0, len(Chrom1[i]))\n",
    "            n2=np.random.randint(0, len(Chrom2[i]))\n",
    "\n",
    "            father=Chrom1[i][n1].copy()\n",
    "            mother=Chrom2[i][n2].copy()\n",
    "\n",
    "            Chrom1[i][n1]=mother\n",
    "            Chrom2[i][n2]=father\n",
    "\n",
    "        Chrom[::2], Chrom[1::2] = Chrom1, Chrom2\n",
    "        self.Chrom = Chrom\n",
    "        return self.Chrom\n",
    "\n",
    "    def mutation(self):\n",
    "        Chrom, size_pop = self.Chrom, self.pop_size\n",
    "\n",
    "        for i in range(size_pop):\n",
    "            n1 = np.random.randint(0, len(Chrom[i]))\n",
    "\n",
    "            # ------------add module---------------\n",
    "            prob = np.random.uniform(0, 1)\n",
    "            if prob < self.add_rate:\n",
    "                add_Chrom = GA.random_module(self)\n",
    "                if add_Chrom not in Chrom[i]:\n",
    "                    Chrom[i].append(add_Chrom)\n",
    "\n",
    "            # --------delete module----------------\n",
    "            prob = np.random.uniform(0, 1)\n",
    "            if prob < self.delete_rate:\n",
    "                if len(Chrom[i]) > 1:\n",
    "                    delete_index = np.random.randint(0, len(Chrom[i]))\n",
    "                    Chrom[i].pop(delete_index)\n",
    "\n",
    "            # ------------gene mutation------------------\n",
    "            prob = np.random.uniform(0, 1)\n",
    "            if prob < self.mutate_rate:\n",
    "                if len(Chrom[i]) > 0:\n",
    "                    n1 = np.random.randint(0, len(Chrom[i]))\n",
    "                    n2 = np.random.randint(0, len(Chrom[i][n1]))\n",
    "                    Chrom[i][n1][n2] = random.randint(0,3)\n",
    "        self.Chrom = Chrom\n",
    "        return self.Chrom\n",
    "\n",
    "\n",
    "    def select(self):  # nature selection wrt pop's fitness\n",
    "        Chrom, size_pop = self.Chrom, self.pop_size\n",
    "        new_Chrom=[]\n",
    "        new_fitness=[]\n",
    "        new_coef=[]\n",
    "        new_name=[]\n",
    "\n",
    "        fitness_list = []\n",
    "        coef_list=[]\n",
    "        name_list=[]\n",
    "\n",
    "        for i in range(size_pop):\n",
    "            gene_translate, length_penalty_coef = GA.translate_DNA(self, Chrom[i])\n",
    "            coef, MSE, MSE_true,name = GA.get_fitness(self, gene_translate, length_penalty_coef)\n",
    "            fitness_list.append(MSE)\n",
    "            coef_list.append(coef)\n",
    "            name_list.append(name)\n",
    "        re1 = list(map(fitness_list.index, heapq.nsmallest(int(size_pop/2), fitness_list)))\n",
    "\n",
    "        for index in re1:\n",
    "            new_Chrom.append(Chrom[index])\n",
    "            new_fitness.append(fitness_list[index])\n",
    "            new_coef.append(coef_list[index])\n",
    "            new_name.append(name_list[index])\n",
    "        for index in range(int(size_pop/2)):\n",
    "            new=GA.random_genome(self)\n",
    "            new_Chrom.append(new)\n",
    "\n",
    "\n",
    "        self.Chrom=new_Chrom\n",
    "        self.Fitness=new_fitness\n",
    "        self.coef=new_coef\n",
    "        self.name=new_name\n",
    "        return self.Chrom,self.Fitness,self.coef,self.name\n",
    "\n",
    "    def delete_duplicates(self):\n",
    "        Chrom, size_pop = self.Chrom,self.pop_size\n",
    "        for i in range(size_pop):\n",
    "            new_genome=[]\n",
    "            for j in range(len(Chrom[i])):\n",
    "                if sorted(Chrom[i][j]) not in new_genome:\n",
    "                    new_genome.append(sorted(Chrom[i][j]))\n",
    "            Chrom[i]=new_genome\n",
    "        self.Chrom=Chrom\n",
    "        return self.Chrom\n",
    "    \n",
    "    def convert_chrom_to_eq(self,chrom,left_name,coef):\n",
    "        name=['u','ux','uxx','uxxx','ut','utt']\n",
    "        string=[]\n",
    "        for i in range(len(chrom)):\n",
    "            item=chrom[i]\n",
    "            string.append(str(np.round(coef[i,0],4)))\n",
    "            string.append('*')\n",
    "            for gene in item:\n",
    "                string.append(name[gene])\n",
    "                string.append('*')\n",
    "            string.pop(-1)\n",
    "            string.append('+')\n",
    "        string.pop(-1)\n",
    "        string=f\"{left_name}=\"+''.join(string)\n",
    "        return string\n",
    "            \n",
    "                \n",
    "\n",
    "    def evolution(self):\n",
    "        self.Chrom = []\n",
    "        self.Fitness=[]\n",
    "        for iter in range(self.pop_size):\n",
    "            intial_genome =GA.random_genome(self)\n",
    "            self.Chrom.append(intial_genome)\n",
    "            gene_translate, length_penalty_coef=GA.translate_DNA(self,intial_genome)\n",
    "            coef, MSE,MSE_true,name=GA.get_fitness(self,gene_translate,length_penalty_coef)\n",
    "            self.Fitness.append(MSE)\n",
    "        GA.delete_duplicates(self)\n",
    "        try:\n",
    "            os.makedirs(f'result_save/{Equation_name}/{choose}_{noise_level}')\n",
    "        except OSError:\n",
    "            pass\n",
    "\n",
    "        with open(f'result_save/{Equation_name}/{choose}_{noise_level}/DLGA_output.txt', \"a\") as f:\n",
    "            date_str = time.strftime('%Y-%m-%d  %H:%M:%S', time.localtime())\n",
    "            f.write(f'\\n============Run at {date_str}==============\\n')\n",
    "            f.write(f'============Params=============\\n')\n",
    "            f.write(f'#l0_penalty:{self.epi}\\n')\n",
    "            f.write(f'#pop_size:{self.pop_size}\\n')\n",
    "            f.write(f'#generations:{self.n_generations}\\n')\n",
    "            f.write(f'============results=============\\n')\n",
    "        for iter in tqdm(range(self.n_generations)):\n",
    "            pickle.dump(self.Chrom.copy()[0],open(f'result_save/{Equation_name}/{choose}_{noise_level}/best_save.pkl','wb'))\n",
    "            best =self.Chrom.copy()[0]\n",
    "            GA.cross_over(self)\n",
    "            GA.mutation(self)\n",
    "            GA.delete_duplicates(self)\n",
    "            best = pickle.load(open(f'result_save/{Equation_name}/{choose}_{noise_level}/best_save.pkl','rb'))\n",
    "            self.Chrom[0]=best\n",
    "            GA.select(self)\n",
    "            if self.Chrom[0]!=best:\n",
    "                with open(f'result_save/{Equation_name}/{choose}_{noise_level}/DLGA_output.txt', \"a\") as f:\n",
    "                    f.write(f'iter: {iter+1}\\n')\n",
    "                    f.write(f'The best Chrom: {self.Chrom[0]}\\n')\n",
    "                    f.write(f'The best coef:  \\n{self.coef[0]}\\n')\n",
    "                    f.write(f'The best fitness: {self.Fitness[0]}\\n')\n",
    "                    f.write(f'The best name: {self.name[0]}\\n')\n",
    "                    f.write(f'----------------------------------------\\n')\n",
    "                    print(f'iter: {iter+1}\\n')\n",
    "                    print(f'The best Chrom: {self.Chrom[0]}')\n",
    "                    print(f'The best coef:  \\n{self.coef[0]}')\n",
    "                    print(f'The best fitness: {self.Fitness[0]}')\n",
    "                    print(f'The best name: {self.name[0]}\\r')\n",
    "        print('-------------------------------------------')\n",
    "        print(f'Finally discovered equation')\n",
    "        print(f'The best Chrom: {self.Chrom[0]}')\n",
    "        print(f'The best coef:  \\n{self.coef[0]}')\n",
    "        print(f'The best fitness: {self.Fitness[0]}')\n",
    "        print(f'The best name: {self.name[0]}\\r')\n",
    "        print('---------------------------------------------')\n",
    "\n",
    "        return self.Chrom[0],self.coef[0],self.Fitness[0],self.name[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "29d7f242-4bc9-4cab-ade3-45e1bc241c1f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dataset size: x_num 512  t_num  201\n"
     ]
    }
   ],
   "source": [
    "Equation_name='KdV_equation'\n",
    "choose=10000\n",
    "noise_level=0\n",
    "if Equation_name=='KdV_equation':\n",
    "    data_path = fr\"KdV_equation.mat\"\n",
    "    data = scio.loadmat(data_path)\n",
    "    un = data.get(\"uu\")\n",
    "    x = np.squeeze(data.get(\"x\"))\n",
    "    t = np.squeeze(data.get(\"tt\").reshape(1, 201))\n",
    "    x_low = -0.8\n",
    "    x_up = 0.8\n",
    "    t_low = 0.1\n",
    "    t_up = 0.9\n",
    "    target = [[3], [0, 1]]\n",
    "    Left = 'u_t'\n",
    "    epi = 1e-1\n",
    "    Delete_equation_name='KdV equation'\n",
    "x_num=x.shape[0]\n",
    "t_num=t.shape[0]\n",
    "total=x_num*t_num\n",
    "choose_validate=5000\n",
    "meta_data_num=10000\n",
    "print(f\"dataset size: x_num {x_num}  t_num  {t_num}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "cdcf4a51-daca-428d-be43-3c091126b66a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===load noisy data===\n"
     ]
    }
   ],
   "source": [
    "# add noise\n",
    "noise_value=(noise_level/100)*np.std(un)*np.random.randn(*un.shape)\n",
    "un=un+noise_value\n",
    "#save model dir\n",
    "\n",
    "try:\n",
    "    os.makedirs(f'noise_data_save/{Equation_name}/')\n",
    "    np.save(f'noise_data_save/{Equation_name}/un_{choose}_{noise_level}.npy', un)\n",
    "except OSError:\n",
    "    un=np.load(f'noise_data_save/{Equation_name}/un_{choose}_{noise_level}.npy')\n",
    "    print('===load noisy data===')\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3b339a36-4e1a-41ae-866c-1a7c787d6470",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training data: torch.Size([10000, 2]) Validating data torch.Size([5000, 2])\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(525)\n",
    "torch.cuda.manual_seed(525)\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device('cuda')\n",
    "else:\n",
    "    device = torch.device('cpu')\n",
    "Net=NN(Num_Hidden_Layers=5,\n",
    "    Neurons_Per_Layer=50,\n",
    "    Input_Dim=2,\n",
    "    Output_Dim=1,\n",
    "    Data_Type=torch.float32,\n",
    "    Device='cuda',\n",
    "    Activation_Function=\"Sin\",\n",
    "    Batch_Norm=False)\n",
    "try:\n",
    "    os.makedirs(f'model_save/{Equation_name}/{choose}_{noise_level}')\n",
    "except OSError:\n",
    "    pass\n",
    "\n",
    "#=========produce random dataset==========\n",
    "h_data_choose,h_data_validate,database_choose,database_validate=random_data(total,choose,choose_validate,x,t,un,x_num,t_num)\n",
    "database_choose = Variable(database_choose.cuda(),requires_grad=True)\n",
    "database_validate = Variable(database_validate.cuda(),requires_grad=True)\n",
    "h_data_choose=Variable(h_data_choose.cuda())\n",
    "h_data_validate=Variable(h_data_validate.cuda())\n",
    "\n",
    "print(\"Training data:\",database_choose.shape,\"Validating data\",database_validate.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5350c12e-1018-415e-bd16-67fcff27c0d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===============train Net=================\n",
      "iter_num: 500      loss: 0.00218572    loss_validate: 0.00240086\n",
      "iter_num: 1000      loss: 0.00048595    loss_validate: 0.00052037\n",
      "iter_num: 1500      loss: 0.00027203    loss_validate: 0.00028582\n",
      "iter_num: 2000      loss: 0.00020035    loss_validate: 0.00020832\n",
      "iter_num: 2500      loss: 0.00017430    loss_validate: 0.00018839\n",
      "iter_num: 3000      loss: 0.00013247    loss_validate: 0.00014521\n",
      "iter_num: 3500      loss: 0.00011370    loss_validate: 0.00012781\n",
      "iter_num: 4000      loss: 0.00009687    loss_validate: 0.00011427\n",
      "iter_num: 4500      loss: 0.00008539    loss_validate: 0.00010617\n",
      "iter_num: 5000      loss: 0.00015732    loss_validate: 0.00018393\n",
      "iter_num: 5500      loss: 0.00006567    loss_validate: 0.00009272\n",
      "iter_num: 6000      loss: 0.00005800    loss_validate: 0.00008577\n",
      "iter_num: 6500      loss: 0.00023031    loss_validate: 0.00024528\n",
      "iter_num: 7000      loss: 0.00004532    loss_validate: 0.00006954\n",
      "iter_num: 7500      loss: 0.00003921    loss_validate: 0.00005989\n",
      "iter_num: 8000      loss: 0.00003437    loss_validate: 0.00005121\n",
      "iter_num: 8500      loss: 0.00009114    loss_validate: 0.00009801\n",
      "iter_num: 9000      loss: 0.00002339    loss_validate: 0.00003256\n",
      "iter_num: 9500      loss: 0.00015188    loss_validate: 0.00015879\n",
      "iter_num: 10000      loss: 0.00001759    loss_validate: 0.00002310\n",
      "iter_num: 10500      loss: 0.00016695    loss_validate: 0.00017257\n",
      "iter_num: 11000      loss: 0.00002451    loss_validate: 0.00002948\n",
      "iter_num: 11500      loss: 0.00001360    loss_validate: 0.00001708\n",
      "iter_num: 12000      loss: 0.00014502    loss_validate: 0.00014090\n",
      "iter_num: 12500      loss: 0.00006227    loss_validate: 0.00006588\n",
      "iter_num: 13000      loss: 0.00001116    loss_validate: 0.00001354\n",
      "iter_num: 13500      loss: 0.00003645    loss_validate: 0.00003511\n",
      "iter_num: 14000      loss: 0.00001699    loss_validate: 0.00001931\n",
      "iter_num: 14500      loss: 0.00068642    loss_validate: 0.00067287\n",
      "iter_num: 15000      loss: 0.00001547    loss_validate: 0.00001702\n",
      "iter_num: 15500      loss: 0.00000875    loss_validate: 0.00001032\n",
      "iter_num: 16000      loss: 0.00000846    loss_validate: 0.00001004\n",
      "iter_num: 16500      loss: 0.00000920    loss_validate: 0.00001082\n",
      "iter_num: 17000      loss: 0.00001363    loss_validate: 0.00001604\n",
      "iter_num: 17500      loss: 0.00019435    loss_validate: 0.00018863\n",
      "iter_num: 18000      loss: 0.00000981    loss_validate: 0.00001158\n",
      "iter_num: 18500      loss: 0.00000760    loss_validate: 0.00000891\n",
      "iter_num: 19000      loss: 0.00000686    loss_validate: 0.00000769\n",
      "iter_num: 19500      loss: 0.00001322    loss_validate: 0.00001319\n",
      "iter_num: 20000      loss: 0.00001042    loss_validate: 0.00001059\n",
      "iter_num: 20500      loss: 0.00000792    loss_validate: 0.00000926\n",
      "iter_num: 21000      loss: 0.00000706    loss_validate: 0.00000742\n",
      "iter_num: 21500      loss: 0.00003673    loss_validate: 0.00003425\n",
      "iter_num: 22000      loss: 0.00001173    loss_validate: 0.00001243\n",
      "iter_num: 22500      loss: 0.00000528    loss_validate: 0.00000582\n",
      "iter_num: 23000      loss: 0.00000888    loss_validate: 0.00001011\n",
      "iter_num: 23500      loss: 0.00001175    loss_validate: 0.00001161\n",
      "iter_num: 24000      loss: 0.00001489    loss_validate: 0.00001667\n",
      "iter_num: 24500      loss: 0.00001127    loss_validate: 0.00001135\n",
      "iter_num: 25000      loss: 0.00000466    loss_validate: 0.00000495\n",
      "iter_num: 25500      loss: 0.00000582    loss_validate: 0.00000613\n",
      "iter_num: 26000      loss: 0.00000944    loss_validate: 0.00000976\n",
      "iter_num: 26500      loss: 0.00000606    loss_validate: 0.00000649\n",
      "iter_num: 27000      loss: 0.00002817    loss_validate: 0.00002943\n",
      "iter_num: 27500      loss: 0.00004413    loss_validate: 0.00004311\n",
      "iter_num: 28000      loss: 0.00000426    loss_validate: 0.00000452\n",
      "iter_num: 28500      loss: 0.00005651    loss_validate: 0.00005417\n",
      "iter_num: 29000      loss: 0.00000359    loss_validate: 0.00000391\n",
      "iter_num: 29500      loss: 0.00002620    loss_validate: 0.00002583\n",
      "iter_num: 30000      loss: 0.00001950    loss_validate: 0.00001994\n",
      "iter_num: 30500      loss: 0.00000489    loss_validate: 0.00000515\n",
      "iter_num: 31000      loss: 0.00001363    loss_validate: 0.00001452\n",
      "iter_num: 31500      loss: 0.00001391    loss_validate: 0.00001441\n",
      "iter_num: 32000      loss: 0.00000326    loss_validate: 0.00000348\n",
      "iter_num: 32500      loss: 0.00002071    loss_validate: 0.00002053\n",
      "iter_num: 33000      loss: 0.00001806    loss_validate: 0.00001876\n",
      "iter_num: 33500      loss: 0.00001993    loss_validate: 0.00002033\n",
      "iter_num: 34000      loss: 0.00005182    loss_validate: 0.00005197\n",
      "iter_num: 34500      loss: 0.00000661    loss_validate: 0.00000638\n",
      "iter_num: 35000      loss: 0.00000989    loss_validate: 0.00000958\n",
      "iter_num: 35500      loss: 0.00000269    loss_validate: 0.00000282\n",
      "iter_num: 36000      loss: 0.00008466    loss_validate: 0.00008315\n",
      "iter_num: 36500      loss: 0.00000269    loss_validate: 0.00000277\n",
      "iter_num: 37000      loss: 0.00000494    loss_validate: 0.00000534\n",
      "iter_num: 37500      loss: 0.00041259    loss_validate: 0.00041294\n",
      "iter_num: 38000      loss: 0.00036414    loss_validate: 0.00035864\n",
      "iter_num: 38500      loss: 0.00000793    loss_validate: 0.00000750\n",
      "iter_num: 39000      loss: 0.00000608    loss_validate: 0.00000594\n",
      "iter_num: 39500      loss: 0.00000301    loss_validate: 0.00000323\n",
      "iter_num: 40000      loss: 0.00000967    loss_validate: 0.00001001\n",
      "iter_num: 40500      loss: 0.00000527    loss_validate: 0.00000515\n",
      "iter_num: 41000      loss: 0.00001325    loss_validate: 0.00001268\n",
      "iter_num: 41500      loss: 0.00000176    loss_validate: 0.00000187\n",
      "iter_num: 42000      loss: 0.00000166    loss_validate: 0.00000179\n",
      "iter_num: 42500      loss: 0.00001144    loss_validate: 0.00001188\n",
      "iter_num: 43000      loss: 0.00000369    loss_validate: 0.00000380\n",
      "iter_num: 43500      loss: 0.00000192    loss_validate: 0.00000205\n",
      "iter_num: 44000      loss: 0.00000148    loss_validate: 0.00000159\n",
      "iter_num: 44500      loss: 0.00000364    loss_validate: 0.00000384\n",
      "iter_num: 45000      loss: 0.00001087    loss_validate: 0.00001121\n",
      "iter_num: 45500      loss: 0.00001292    loss_validate: 0.00001284\n",
      "iter_num: 46000      loss: 0.00000261    loss_validate: 0.00000260\n",
      "iter_num: 46500      loss: 0.00000166    loss_validate: 0.00000176\n",
      "iter_num: 47000      loss: 0.00008549    loss_validate: 0.00008740\n",
      "iter_num: 47500      loss: 0.00000147    loss_validate: 0.00000154\n",
      "iter_num: 48000      loss: 0.00001309    loss_validate: 0.00001332\n",
      "iter_num: 48500      loss: 0.00000124    loss_validate: 0.00000133\n",
      "iter_num: 49000      loss: 0.00000373    loss_validate: 0.00000373\n",
      "iter_num: 49500      loss: 0.00000333    loss_validate: 0.00000341\n",
      "iter_num: 50000      loss: 0.00000441    loss_validate: 0.00000469\n",
      "48500\n"
     ]
    }
   ],
   "source": [
    "NN_optimizer = torch.optim.Adam([\n",
    "        {'params': Net.parameters()},\n",
    "    ])\n",
    "\n",
    "\n",
    "MSELoss = torch.nn.MSELoss()\n",
    "validate_error=[]\n",
    "best_validate_error = []\n",
    "loss_back = 1e8\n",
    "flag = 0\n",
    "print(f'===============train Net=================')\n",
    "file = open(f'model_save/{Equation_name}/{choose}_{noise_level}/loss.txt', 'w').close()\n",
    "file=open(f'model_save/{Equation_name}/{choose}_{noise_level}/loss.txt',\"a+\")\n",
    "for iter in range(50000):\n",
    "    NN_optimizer.zero_grad()\n",
    "    prediction = Net(database_choose)\n",
    "    prediction_validate = Net(database_validate).cpu().data.numpy()\n",
    "    loss = MSELoss(h_data_choose, prediction)\n",
    "    loss_validate = np.sum((h_data_validate.cpu().data.numpy() - prediction_validate) ** 2) / choose_validate\n",
    "    loss.backward()\n",
    "    NN_optimizer.step()\n",
    "\n",
    "\n",
    "\n",
    "    if (iter+1) % 500 == 0:\n",
    "        validate_error.append(loss_validate)\n",
    "        torch.save(Net.state_dict(),\n",
    "                   f'model_save/{Equation_name}/{choose}_{noise_level}/' + f\"Net_{iter + 1}.pkl\")\n",
    "\n",
    "        print(\"iter_num: %d      loss: %.8f    loss_validate: %.8f\" % (iter+1, loss, loss_validate))\n",
    "        file.write(\"iter_num: %d      loss: %.8f    loss_validate: %.8f \\n\" % (iter+1, loss, loss_validate))\n",
    "file.close()\n",
    "best_epoch=(validate_error.index(min(validate_error))+1)*500\n",
    "print(best_epoch)\n",
    "np.save(f'model_save/{Equation_name}/{choose}_{noise_level}/best_epoch.npy',np.array([best_epoch]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "282c5ef2-38ca-4705-9f21-da6e0830aab5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "best_epoch: 48500\n",
      "(10000, 6)\n"
     ]
    }
   ],
   "source": [
    "best_epoch=np.load(f'model_save/{Equation_name}/{choose}_{noise_level}/best_epoch.npy')[0]\n",
    "print(\"best_epoch:\",best_epoch)\n",
    "Load_state = 'Net' +  f'_{best_epoch}'\n",
    "Theta,x_meta,t_meta=Generate_meta_data(Net,Equation_name,choose,noise_level,Load_state,x_low,x_up,t_low,t_up)\n",
    "print(Theta.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e319f3e5-6e45-4400-959a-6a99fbe31883",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|▊                                                                                 | 1/100 [00:00<01:05,  1.50it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter: 1\n",
      "\n",
      "The best Chrom: [[3], [0, 1]]\n",
      "The best coef:  \n",
      "[[-0.00248602]\n",
      " [-0.99451355]]\n",
      "The best fitness: 0.33101108574304633\n",
      "The best name: u_t\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  5%|████                                                                              | 5/100 [00:02<00:53,  1.77it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter: 5\n",
      "\n",
      "The best Chrom: [[0, 1], [3]]\n",
      "The best coef:  \n",
      "[[-0.99451355]\n",
      " [-0.00248602]]\n",
      "The best fitness: 0.33101108574304505\n",
      "The best name: u_t\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████| 100/100 [01:02<00:00,  1.60it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------\n",
      "Finally discovered equation\n",
      "The best Chrom: [[0, 1], [3]]\n",
      "The best coef:  \n",
      "[[-0.99451355]\n",
      " [-0.00248602]]\n",
      "The best fitness: 0.33101108574304505\n",
      "The best name: u_t\n",
      "---------------------------------------------\n",
      "equation form: u_t=-0.9945*u*ux+-0.0025*uxxx\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "opt_GA=GA(x_meta,t_meta,epi,Theta)\n",
    "Chrom,coef,_,name=opt_GA.evolution()\n",
    "print(\"equation form:\",opt_GA.convert_chrom_to_eq(Chrom,name,coef))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
